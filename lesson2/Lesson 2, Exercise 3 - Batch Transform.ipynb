{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Upload (again, again) to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://edgarin-mlend-c2/toys/instruments/music_instruments_reviews.txt\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Input the s3 bucket\n",
    "BUCKET = \"edgarin-mlend-c2\"\n",
    "# Input the s3 prefix\n",
    "s3_prefix = \"toys/instruments\"\n",
    "# Input the the file to write the data to\n",
    "file_name = \"music_instruments_reviews.txt\"\n",
    "\n",
    "\n",
    "def unzip_data(input_data_path):\n",
    "    with zipfile.ZipFile(input_data_path, 'r') as input_data_zip:\n",
    "        input_data_zip.extractall('.')\n",
    "\n",
    "def split_sentences(input_data):\n",
    "    split_sentences = []\n",
    "    for l in open(input_data, 'r'):\n",
    "        l_object = json.loads(l)\n",
    "        helpful_votes = float(l_object['helpful'][0])\n",
    "        total_votes = l_object['helpful'][1]\n",
    "        if total_votes != 0 and helpful_votes/total_votes != .5:  # Filter out same data as prior jobs. \n",
    "            reviewText = l_object['reviewText']\n",
    "            sentences = reviewText.split(\".\") \n",
    "            for s in sentences:\n",
    "                if s: # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                    split_sentences.append(s)\n",
    "    return split_sentences\n",
    "\n",
    "# Format the data as  {'source': 'THIS IS A SAMPLE SENTENCE'}\n",
    "# And write the data into a file\n",
    "def cycle_data(fp, data):\n",
    "    for d in data:\n",
    "        fp.write(json.dumps({'source':d}) + '\\n')\n",
    "\n",
    "# upload the data to s3\n",
    "def upload_file_to_s3(file_name, s3_prefix):\n",
    "    object_name = os.path.join(s3_prefix, file_name)\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, BUCKET, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "\n",
    "# Unzip archive\n",
    "unzip_data('reviews_Musical_Instruments_5.json.zip')\n",
    "\n",
    "# Preprocess reviews_Musical_Instruments_5.json\n",
    "sentences = split_sentences('reviews_Musical_Instruments_5.json')\n",
    "\n",
    "# Write data to a file and upload it to s3.   \n",
    "with open(file_name, 'w') as f:\n",
    "    cycle_data(f, sentences)\n",
    "\n",
    "upload_file_to_s3(file_name, s3_prefix)\n",
    "\n",
    "# Get the s3 path for the data\n",
    "batch_transform_input_path = \"s3://\" +  \"/\".join([BUCKET, s3_prefix, file_name])\n",
    "\n",
    "print(batch_transform_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution: Use Batch Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................\n",
      "\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[12/02/2021 14:15:45 INFO 140363024639360] Finding and loading model\u001b[0m\n",
      "\u001b[34m[12/02/2021 14:15:45 INFO 140363024639360] Trying to load model from /opt/ml/model/model.bin\u001b[0m\n",
      "\u001b[34m[12/02/2021 14:15:46 INFO 140363024639360] Number of server workers: 4\u001b[0m\n",
      "\u001b[35mArguments: serve\u001b[0m\n",
      "\u001b[35m[12/02/2021 14:15:45 INFO 140363024639360] Finding and loading model\u001b[0m\n",
      "\u001b[35m[12/02/2021 14:15:45 INFO 140363024639360] Trying to load model from /opt/ml/model/model.bin\u001b[0m\n",
      "\u001b[35m[12/02/2021 14:15:46 INFO 140363024639360] Number of server workers: 4\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [35] [INFO] Booting worker with pid: 35\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [36] [INFO] Booting worker with pid: 36\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [35] [INFO] Booting worker with pid: 35\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [36] [INFO] Booting worker with pid: 36\u001b[0m\n",
      "\u001b[32m2021-12-02T14:15:49.783:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[12/02/2021 14:15:45 INFO 140363024639360] Finding and loading model\u001b[0m\n",
      "\u001b[34m[12/02/2021 14:15:45 INFO 140363024639360] Trying to load model from /opt/ml/model/model.bin\u001b[0m\n",
      "\u001b[34m[12/02/2021 14:15:46 INFO 140363024639360] Number of server workers: 4\u001b[0m\n",
      "\u001b[35mArguments: serve\u001b[0m\n",
      "\u001b[35m[12/02/2021 14:15:45 INFO 140363024639360] Finding and loading model\u001b[0m\n",
      "\u001b[35m[12/02/2021 14:15:45 INFO 140363024639360] Trying to load model from /opt/ml/model/model.bin\u001b[0m\n",
      "\u001b[35m[12/02/2021 14:15:46 INFO 140363024639360] Number of server workers: 4\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [35] [INFO] Booting worker with pid: 35\u001b[0m\n",
      "\u001b[34m[2021-12-02 14:15:46 +0000] [36] [INFO] Booting worker with pid: 36\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [1] [INFO] Using worker: sync\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [35] [INFO] Booting worker with pid: 35\u001b[0m\n",
      "\u001b[35m[2021-12-02 14:15:46 +0000] [36] [INFO] Booting worker with pid: 36\u001b[0m\n",
      "\u001b[32m2021-12-02T14:15:49.783:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "image_uri = image_uris.retrieve(framework='blazingtext',region='us-east-1')\n",
    "\n",
    "model_data = \"s3://edgarin-mlend-c2/toys/mi-output/toy-reviews-training-job/output/model.tar.gz\"\n",
    "\n",
    "batch_transform_output_path = \"s3://edgarin-mlend-c2/toys/instruments/nb-batch-output\"\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=model_data, role=role)\n",
    "\n",
    "transformer = model.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m4.xlarge', \n",
    "    output_path=batch_transform_output_path    \n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data=batch_transform_input_path, \n",
    "    data_type='S3Prefix',\n",
    "    content_type='application/jsonlines', \n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
